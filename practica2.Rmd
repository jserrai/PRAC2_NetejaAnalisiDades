---
title: "practica2"
author: "Paula Jordi"
date: "30/12/2020"
output: html_document
---

```{r setup, include=FALSE,message= FALSE, warning=FALSE} 
knitr::opts_chunk$set(echo = TRUE)
library(lubridate)
library(ggplot2)
library(patchwork)
library(dplyr)
library(scales)
library(Hmisc)
library(gridExtra)
library(ggpubr)
library(raster)
library(psych)
library(car)
```

## 1. Integració i selecció de les dades d’interès a analitzar:

Obtenim les dades del repositori: https://www.kaggle.com/lantanacamara/hong-kong-horse-racing
Aquest conté dades dels resultats de les 1561 curses de cavalls fetes a Hong Kong entre el 14 de Setembre del 2014 i el 16 de Juliol de 2017. Aquest està organitzat amb dos fitxers: race-result-horse.csv i race-result-race.csv:

```{r message= FALSE, warning=FALSE}
horse.data <- read.csv("./race-result-horse.csv")
race.data <- read.csv("./race-result-race.csv")
```

Unifiquem els dos datasets en un a partir de la variable comú als dos fitxers. Aquesta fa referencia al identificador de la carrera: __race_id__

```{r message= FALSE, warning=FALSE}
dataset <- merge(horse.data, race.data, by = "race_id")
dim(dataset)
```


## 2. Neteja de les dades:

Abans de netejar les dades, hem de mirar què hi tenim a cada columna. Per fer-ho utilizarem la funció summary() que ens permet visualitzar i fer-nos una idea general general el dataset: 

```{r message= FALSE, warning=FALSE}
#head(dataset,5)
str(dataset)
paste("Nombre de variables:" ,length(colnames(dataset)))
```

Com podem veure, tenim 30 variables amb 30189 observacions. Com que hi ha moltes variables que estan repetides o que no aporten una informació vàlida, n'eliminarem algunes:

1. __incident_report__: Són comentaris sobre la cursa, no utilitzarem aquesta variable per als nostres anàlisis, per tant els podem eliminar.
2. __running_position_X__: Entenem que són les posicions intermitjes de la carrera. No ens aporta informació sobre abans de la carrera, sinó durant aquesta.
3. __SRC__: Tampoc ens aporta res més que el nom dels fitxers de dades.
4. __horse_number__: Depèn de cada cursa i no identifica el cavall, per tant deixem només el __horse_name__.
5. __race_number__: També és reduntant amb les altres variables.
6. __race_date__: La data de la carrera és indiferent per l'anàlisi de les dades. 
7. __sectional_time__: Tindrem en compte només el temps total de la carrera, no de cada volta.
8. __horse_id__ : Ja tenim un identificador pel cavall, aques és redundant.
9. __length_behind_winner__ : És redundant, ja tenim la posició final de la cursa.

```{r message= FALSE, warning=FALSE}
eliminar <- c("running_position_1","running_position_2","running_position_3","running_position_4","running_position_5","running_position_6","incident_report","src","race_date","race_number","horse_number","horse_id","sectional_time","length_behind_winner")
dataset_clean <- dataset[ , !(names(dataset) %in% eliminar)]
dataset_clean[] <- lapply(dataset_clean, as.character)
attach(dataset_clean) # Per poder fer referencia directament a les variables
```


### 2.1. Les dades contenen zeros o elements buits? Com gestionaries aquests casos?


Fent una ullada al dataset, veiem que alguns valors NA estan representats com a "---", "N", "SH" o "HD". Canviarem aquests valors a NA per a poder treballar amb la mateixa notació. 
A més a més, a la columna de __length_behind_winner__ (la distància del cavall en qüestió en relació al primer), ens trobem valors representats com a "-". Aquests corresponen al guanyador de la cursa, el quel no té distància sobre sí mateix, és a dir no són valors buits sinó que representen distància 0. Modificarem la notació per "0".
A la variable __finishing_position__, tenim diversos valors no numèrics, aquests estan representats amb diferents lletres. Per a utilitzar la mateixa notació, canviarem la variable a numèrica i aquells valors no numèrics passaran a estar representats amb "NA".

```{r echo=TRUE, message=FALSE, warning=FALSE}
dataset_clean[dataset_clean == "-"] <- "0"
dataset_clean[dataset_clean == "---"] <- NA
dataset_clean[dataset_clean == "N"] <- NA
dataset_clean[dataset_clean == "SH"] <- NA
dataset_clean[dataset_clean == "HD"] <- NA
dataset_clean$finishing_position  <- lapply(dataset_clean$finishing_position, as.character)
dataset_clean$finishing_position <- unlist(lapply(dataset_clean$finishing_position, as.integer)) # tots aquells valors que no s'hagin pogut transformar són els no numèrics, i restaran com a "NA".
```

Un cop ja tenim tots els valors buits amb la mateixa identificació, podem mirar si tenim gaires elements buits, i en quines variables.

```{r echo=TRUE, message=FALSE, warning=FALSE}
#paste("Nombre de valors nulls:", sum(is.na(race_id)))
colSums(is.na(dataset_clean))
```

Podem observar que tenim alguns valors buits al nostre dataset. Primer ens centrarem en la variable __finishing_position__. Determinem quin % de valors buits representa, per a poder decidir com actuem:

```{r message= FALSE, warning=FALSE}
paste("Proporció de valors nulls de la variable finishing_position:", format((sum(is.na(dataset_clean$finishing_position))/nrow(dataset_clean))*100, format = "f", digits = 3), "%")
```

Veiem que representa un valor molt baix de totes les nostres dades, així que aquest cas el podem gestionar eliminant les observacions nules. Si representés una proporció més gran de les nostres dades, al voltant de 15-20% hauriem d'imputar els missing values, utilitzant un anàlisi de regressió. 
Observem quans valors nuls tenim ara:

```{r message= FALSE, warning=FALSE}
dataset_clean <- dataset_clean[!(is.na(dataset_clean$finishing_position)),] # eliminem les files sense resultats.
colSums(is.na(dataset_clean))
```


Un op netejada la variable, ja no tenim més valors buits en les altres columnes.
Tot i així, si mirem al dataset original, veiem que hi havia bastants valors buits. Aquests, estaven concentrats en les columnes que hem eliminat perque no ens aportaven informació útil.

```{r echo=TRUE, message=FALSE, warning=FALSE}
#summary(dataset[,-30])
colSums(is.na(dataset[,eliminar]))
```

### 2.2. Identificació i tractament de valors extrems

Per a visualitzar els valors extrems, primer escollirem aquelles variables en les quals podem trobar-nos valors extrems.
De totes les variables del nostre dataset, només les variables continues poden tenir valors extrems, aquelles que categòriques o identificatives no en tindran ( __race_id__, __horse_name__, __jockey__, __trainer__, __draw__, __race_course__, __race_name__, __race_class__, __track__, __track_condition__, __finishing_position__, __length_behind_winner__,__race_distance__ ).
Primer transformem les variables continues a numèriques per poder trobar-ne els outliers.

```{r echo=TRUE, message=FALSE, warning=FALSE}
dataset_clean$actual_weight <- unlist(lapply(dataset_clean$actual_weight, as.numeric))
dataset_clean$declared_horse_weight <- unlist(lapply(dataset_clean$declared_horse_weight, as.numeric))
dataset_clean$finish_time <-  period_to_seconds(ms(dataset_clean$finish_time))
dataset_clean$win_odds <- unlist(lapply(dataset_clean$win_odds, as.numeric))
dataset_clean$race_distance <- as.factor(dataset_clean$race_distance)
```


> **Visualitzem els outliers:**
En el nostre dataset, hi tenim diverses curses de cavalls, aquestes tenen diferents distàncies de 1000m a 2400m. Segons amb quines variables hem de determinar els outliers tenint en compte la distància de la cursa, sinó aquest factor crearà un bias en els costres resultats.
Utilitzem una representació gràfica (histograma i boxplot), per a visualitzar i detectar els outliers:

```{r echo=TRUE, message=FALSE, warning=FALSE, fig.width=10, fig.height=15}
hist_f <- ggplot(data = dataset_clean, aes(x=dataset_clean$finish_time)) + 
  facet_grid(rows = vars(dataset_clean$race_distance), scale = "free_y") +
  geom_histogram(binwidth=1, fill="#0c4c8a") +
  theme_minimal() +
  ggtitle("Histograma - Finnish time")
hist_aw <- ggplot(data = dataset_clean, aes(x=dataset_clean$actual_weight)) + 
  geom_histogram(binwidth=1, fill="#0c4c8a") +
  theme_minimal() +
  ggtitle("Histograma - Actual Weight")
hist_dw <- ggplot(data = dataset_clean, aes(x=dataset_clean$declared_horse_weight)) + 
  geom_histogram(binwidth=1, fill="#0c4c8a") +
  theme_minimal() +
  ggtitle("Histograma - Declared Weight")
hist_wo <- ggplot(data = dataset_clean, aes(x=dataset_clean$win_odds)) + 
  facet_grid(rows = vars(dataset_clean$race_distance), scale = "free_y") +
  geom_histogram(binwidth=1, fill="#0c4c8a") +
  theme_minimal() +
  ggtitle("Histograma - Win Odds")
box_f <- ggplot(dataset_clean) +
  aes(x = "", y = dataset_clean$finish_time) +
  facet_grid(cols = vars(dataset_clean$race_distance), scale = "free") +
  geom_boxplot(fill = "#0c4c8a", outlier.colour = "red") +
  ggtitle("Boxplot - Finnish time") +
  ylab("") + xlab("race distance") + 
  theme_minimal()
box_aw <- ggplot(dataset_clean) +
  aes(x = "", y = dataset_clean$actual_weight) +
  geom_boxplot(fill = "#0c4c8a",outlier.colour = "red") +
  ggtitle("Boxplot - Actual Weight") +
  ylab("") + xlab("race distance") +
  theme_minimal()
box_dw <- ggplot(dataset_clean) +
  aes(x = "", y = dataset_clean$declared_horse_weight) +
  geom_boxplot(fill = "#0c4c8a",outlier.colour = "red") +
  ggtitle("Boxplot - Declared weight") +
  ylab("") + xlab("race distance") +
  theme_minimal()
box_wo <- ggplot(dataset_clean) +
  aes(x = "", y = dataset_clean$win_odds) +
  facet_grid(cols = vars(dataset_clean$race_distance), scale = "free") +
  geom_boxplot(fill = "#0c4c8a",outlier.colour = "red") +
  ggtitle("Boxplot - Win Odds") +
  ylab("") + xlab("race distance") +
  theme_minimal()
hist_f + hist_aw + hist_dw + hist_wo + box_f + box_aw + box_dw + box_wo + plot_layout(nrow = 4, ncol = 2)
```

> **Ara eliminem els outliers de cada una de les variables:**
Eliminarem tots aquells valors outliers. Tot i així, podem observar que a la variable _win_odds_, tots els outliers són els cavalls que tenen més probabilitats de guanyar. Com que el nostre estudi té a veure amb aquests, no eliminarem aquests outliers del nostre dataset i els tindrem en compte pels anàlisis posteriors.

```{r echo=TRUE, message=FALSE, warning=FALSE}
out_aw <- unlist(ggplot_build(box_aw)[["data"]][[1]][["outliers"]])
ind_aw <- which(dataset_clean$actual_weight %in% c(out_aw))
out_f <- unlist(ggplot_build(box_f)[["data"]][[1]][["outliers"]])
ind_f <- which(dataset_clean$finish_time %in% c(out_f))
out_dw <- unlist(ggplot_build(box_dw)[["data"]][[1]][["outliers"]])
ind_dw <- which(dataset_clean$declared_horse_weight %in% c(out_dw))
dataset_outliers <- dataset_clean[-c(ind_f,ind_dw,ind_aw),]
paste("Hem eliminat", (nrow(dataset_clean)-nrow(dataset_outliers)) , "outliers.")
paste("Ara el nostre dataset té", nrow(dataset_outliers), "observacions.")
dataset_clean <- dataset_outliers
```

## 3. Anàlisi de les dades.

### 3.1.  Selecció dels grups de dades que es volen analitzar/comparar (planificació dels anàlisis a aplicar).

### 3.2.  1. Comprovació de la normalitat i homogeneïtat de la variància.

Anem a veure la normalitat i homogeneitat de la variança.

Per a comporvar la normalitat i la homogeneitat, mirarem com estan distribuides les dades, per a comprovar si compleixen amb la hipotesi de la distribució normal de les dades, i si es comporten de manera homogenia. Ho farem de dues maneres: a) De manera visual. Amb un QQPlot, que ens permetrà veure si les dades s'ajusten a la normalitat, i amb un scatter plot que ens ajudarà a veure si és comporten de manera homogènia. b) Utiltzant tests específics Per comporvar la normalitat usarem el Test de Shapiro–Wilk, el qual és considerat un dels test més potents per a contrastar la normalitat. Per comporvar la homogeneitat, usarem el test de Bartlett, el qual ens permetrà comparar entre els diversos grups alhora.

Les nostres dades estan mesurades amb diferents llargades de curses, aquest fet, altera els valors d'algunes variables (per exemple, el temps d'una cursa de 1000m serà molt diferent al d'una cursa de 2400m) per tant, per a analitzar la shape de les dades caldrà separar-les per llargada. Es a dir, que farem l'estudi per separat de cada tipus de cursa per poder diferenciar bé el comportament.

#### 1. Finish_time

```{r } 

ggqqplot(data=dataset_clean, x="finish_time", color = "race_distance", ggtheme = theme_minimal(), conf.int = TRUE, size =0.1, title = "QQPlot - Finish Time")
SW <- list()
llargada = c(1000,1200,1400,1600,1800,2000,2200,2400)
i = 1
for (ll in llargada) {
  dades <- dataset_clean[dataset_clean$race_distance == ll,9]
  if (length(dades) < 5000) {
    SW[[i]] <- shapiro.test(dades)
    }
  else{
    SW[[i]] <- shapiro.test(dades[1:5000])
    }
  i = i+1
}
paste("La mitjana del valor W del Test de Shapiro–Wilk:" , mean(c(SW[[1]]$statistic[[1]],SW[[2]]$statistic[[1]],SW[[2]]$statistic[[1]],SW[[2]]$statistic[[1]],SW[[2]]$statistic[[1]],SW[[2]]$statistic[[1]],SW[[2]]$statistic[[1]],SW[[2]]$statistic[[1]])))
paste("El p-value mitjà del Test de Shapiro–Wilk:", mean(c(SW[[1]]$p.value[[1]],SW[[2]]$p.value[[1]],SW[[2]]$p.value[[1]],SW[[2]]$p.value[[1]],SW[[2]]$p.value[[1]],SW[[2]]$p.value[[1]],SW[[2]]$p.value[[1]],SW[[2]]$p.value[[1]])))

```

Podem veure que en el QQPlot, les dades s'ajusten molt bé a la normalitat.
A més el Test de Shapiro–Wilk també ens indica que les nostres dades són normalitzades, el p-value és molt significant i el valor de Shapiro (W) és quasi 1. 


#### 2. Actual Weight

```{r }
dades <- sample(seq(1,nrow(dataset_clean), by =1), 5000)
ggqqplot(data=dataset_clean, x="actual_weight", ggtheme = theme_minimal(), col = "#0c4c8a", conf.int = TRUE, size = 0.5, title = "QQPlot - Actual Weight")
shapiro.test(dataset_clean$actual_weight[dades])
```

En aquets cas, al qqplot veiem que les dades segueixen la linia de la normalitat, tot i que cap al final s'estanquen al voltant de 132 jin (mesura de Xina ~= 65 kg).
Segons el Test de Shapiro–Wilk, veiem que les dades també estan normalitzades, el p-value és molt significant i el valor de Shapiro (W) és quasi 1. 

#### 3. Declared Horse Weight

```{r }
dades <- sample(seq(1,nrow(dataset_clean), by =1), 5000)
ggqqplot(data=dataset_clean, x="declared_horse_weight", ggtheme = theme_minimal(), col = "#0c4c8a", conf.int = TRUE, size = 0.5, title = "QQPlot - Declared Horse Weight")
shapiro.test(dataset_clean$declared_horse_weight[dades])
```

En aquets cas, al qqplot veiem que les dades s'ajusten molt bé a la linia de la normalitat.
Segons el Test de Shapiro–Wilk, veiem que les dades també estan normalitzades, el p-value és molt significant i el valor de Shapiro (W) és quasi 1. 


#### 4. Win Odds

```{r }
dades <- sample(seq(1,nrow(dataset_clean), by =1), 5000)
ggqqplot(data=dataset_clean, x="win_odds", ggtheme = theme_minimal(), col = "#0c4c8a", conf.int = TRUE, size = 0.5, title = "QQPlot - Win Odds")
shapiro.test(dataset_clean$win_odds[dades])
```

En aquest cas, les dades no segueixen una forma normal. Al QQPlot, podem observar que les porbabilitats de guanyar estan repartides de manera "biestable", tant al 0% com al 100%, i molt poques entremig.
Pel que fa al Test de Shapiro–Wilk, el valor (W) és molt més baix que els anteriors (0.76) no correspon a la normalització. Tenim un p-value molt signficant. 





Homogeneitat:

```{r label="Homogeneitat"}
#BT = bartlett.test(dataset_clean$finish_time ~ interaction(dataset_clean$actual_weight, dataset_clean$win_odds,dataset_clean$declared_horse_weight), data = dataset_clean )
#plot(finish_time ~ actual_weight, data = dataset_clean)
BT = bartlett.test(finish_time ~ actual_weight, data = dataset_clean )
print(BT)
dataset_numerical <- select_if(dataset_clean, is.numeric)

dataset_1200 <- dataset_numerical[dataset_clean$race_distance == "1200",]

pairs(dataset_1200, pch = 19,  cex = 0.5,lower.panel=NULL)

pairs(dataset_1200, 
             method = "pearson", # correlation method
             hist.col = "#00AFBB",
             density = TRUE,  # show density plots
             ellipses = TRUE # show correlation ellipses
             )
```

```{r label="Homogeneitat"}
#BT = bartlett.test(dataset_clean$finish_time ~ interaction(dataset_clean$actual_weight, dataset_clean$win_odds,dataset_clean$declared_horse_weight), data = dataset_clean )
plot(finish_time ~ actual_weight, data = dataset_clean)
BT = bartlett.test(finish_time ~ actual_weight, data = dataset_clean )
print(BT)
```




#No funciona aquest apartat, tot i que l'anterior si, mirar el número d'observacions per cada una.
```{r} 
plot(finish_time ~ declared_horse_weight, data = dataset_clean)
#BT = bartlett.test(finish_time ~  declared_horse_weight, data = dataset_clean )
print(BT)
```



## així funciona, amb la llista,  però no sé si es el que es vol aconseguir
#BT = bartlett.test(list(dataset_clean$finish_time,dataset_clean$declared_horse_weight))
#print(BT)
#BT = bartlett.test(list(dataset_clean$finish_time,dataset_clean$actual_weight,dataset_clean$declared_horse_weight))
#print(BT)
#BT = bartlett.test(list(dataset_clean$actual_weight,dataset_clean$declared_horse_weight))
#print(BT)


### 4.3. Aplicació de proves estadístiques per comparar els grups de dades. En funció de les dades i de l’objectiu de l’estudi, aplicar proves de contrast d’hipòtesis, correlacions, regressions, etc. Aplicar almenys tres mètodes d’anàlisi diferents.

> Correlació:

Per a veure les correlacions entre les variables numèriques utilitzarem la correlació de Pearson:

Primer, rescalem els dos pesos (actual_weight) i (declared_horse_weight) a variables entre 0 i 1, per a poder-les comparar. 

```{r label="1"}
dataset_clean$actual_weight<-rescale(dataset_clean$actual_weight)
dataset_clean$declared_horse_weight<-rescale(dataset_clean$declared_horse_weight)
```

Mirem la correlación entre els valors que són numerics, treiem en aquest cas la variable que es vol predir, que es el temps en que ha d'acabar un cavall, i fem la predicció separant per cada una de les diferents carreres. Com ja hem comentat anteriorment, aquestes no es comporten igual i com podem veure hi ha una correlació directe entre el temps de la cursa i la distancia:

```{r label="corelacio"}
cor(x=dataset_clean$finish_time, y=as.numeric(dataset_clean$race_distance))
```


mirem la resta de variables (treient la variable que tenim correlacionada amb el temps)
```{r label="rcorr" }
#eliminar <- c("race_distance")
#dataset_clean <- dataset_clean[ , !(names(dataset_clean) %in% eliminar)]
num <- select_if(dataset_clean, is.numeric)
cat <- select_if(dataset_clean, is.character)
cat[] <- lapply(cat, as.factor)
for (name in colnames(cat)) {
  cat[name] <- unlist(lapply(cat[name], as.numeric))
}
dataset_numerical <- data.frame(num,cat)
cor <- rcorr(as.matrix(dataset_numerical), type = "pearson")
heatmap(cor$r)



```

en aquest cas veiem com la variable _win_odds_ (la provabilitat de guanyar abans d'iniciar la cursa) té una correlació força alta amb la posició final de la cursa, cosa que es pot entendre per ser un bon cavall.

En aquest cas, la posició final de la cursa dependrà molt de la resta de cavalls que hi hagui i no del propi cavall, per tant, també es podria eliminar ja que no afecta a la predicció del resultat, sinó al resultat de cada cursa.

> Regressió:


